# Smart City - Autonomous Vehicle Simulation

![Unity](https://img.shields.io/badge/Unity-2021.3.16f1-black?logo=unity)
![ML-Agents](https://img.shields.io/badge/ML--Agents-2.3.0-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-1.13.1-red?logo=pytorch)
![C#](https://img.shields.io/badge/C%23-10-239120?logo=c-sharp)
![License](https://img.shields.io/badge/License-Academic-green)

A Unity-based reinforcement learning simulation exploring autonomous vehicle navigation in urban environments without traffic signals. This project demonstrates how collective behavior patterns observed in nature (flocking of birds and fish) can be applied to coordinated autonomous vehicle movement.

## üìö Academic Context

This project was developed as a Master's thesis at **Georgian Technical University** (February 2023) for the Computer Science program. The research validates the theory that autonomous vehicles can safely navigate complex intersections without traditional traffic control systems through machine learning and collective behavior patterns.

**Thesis Title:** Autonomous Vehicle Simulation in Urban Environment  
**Author:** Nikoloz Astamidze  
**Program:** Informatics (Code: 0613)  
**Supervisor:** Professor Merab Akhobadze  
**Reviewer:** Professor Mariam Chkhaidze

### üìä Defense Presentation

**[View Master's Defense Presentation (English)](https://docs.google.com/presentation/d/1NOcKFnHB3iI_q2XVUcLx_OZ09nRyxu88X45hr9WY3zg)**

The defense presentation provides a visual overview of the research methodology, experimental results, and key findings.

---

## üéØ Project Overview

### Core Concept

The simulation tests whether autonomous vehicles can adopt decentralized, nature-inspired navigation strategies similar to how flocks of birds or schools of fish move cohesively without central coordination. Instead of relying on traffic lights and road signs, vehicles learn to:

- Perceive their environment through simplified visual sensors
- Make real-time navigation decisions
- Avoid collisions with other vehicles and obstacles
- Follow road paths efficiently
- Coordinate movements collectively

### Key Innovation

**Grid Sensor Vision System:** Agents perceive the environment as colored pixels rather than complex 3D geometry:
- üü¢ **Green:** Reward ball (path guidance)
- üî¥ **Red:** Sidewalks and boundaries
- üîµ **Blue:** Other vehicles

This simplified perception enables faster training while maintaining effective navigation capabilities.

---

## üèóÔ∏è Architecture

### System Components

```
SmartCity/
‚îú‚îÄ‚îÄ ML Training System
‚îÇ   ‚îú‚îÄ‚îÄ TeamDriverAgent.cs        # Main RL agent implementation
‚îÇ   ‚îú‚îÄ‚îÄ MLTrainingScene.cs        # Multi-agent training orchestration
‚îÇ   ‚îî‚îÄ‚îÄ Reward system             # Behavior shaping logic
‚îÇ
‚îú‚îÄ‚îÄ Car Control System
‚îÇ   ‚îú‚îÄ‚îÄ CarController.cs          # Vehicle physics and movement
‚îÇ   ‚îú‚îÄ‚îÄ CarPercepts.cs            # Collision and trigger detection
‚îÇ   ‚îî‚îÄ‚îÄ ICarAgent.cs              # Agent interface contract
‚îÇ
‚îú‚îÄ‚îÄ Perception System
‚îÇ   ‚îú‚îÄ‚îÄ Grid Sensors              # Simplified visual perception
‚îÇ   ‚îî‚îÄ‚îÄ Detectable Objects        # Environment pixelization
‚îÇ
‚îú‚îÄ‚îÄ Navigation System
‚îÇ   ‚îú‚îÄ‚îÄ PathCrawler.cs            # Path following logic
‚îÇ   ‚îú‚îÄ‚îÄ NodePath.cs               # Waypoint management
‚îÇ   ‚îî‚îÄ‚îÄ Connected path network    # Road infrastructure
‚îÇ
‚îî‚îÄ‚îÄ Environment
    ‚îú‚îÄ‚îÄ Road pieces               # Modular road segments
    ‚îú‚îÄ‚îÄ Intersections             # 3-way and 4-way junctions
    ‚îî‚îÄ‚îÄ Training scenarios        # Progressive difficulty levels
```

### Design Principles

Following **Clean Architecture** and **SOLID** principles:
- **Separation of Concerns:** Domain logic isolated from Unity engine dependencies
- **Interface Segregation:** `ICarAgent` defines clear agent contracts
- **Dependency Injection:** Components referenced through serialized fields
- **Single Responsibility:** Each class handles one specific aspect
- **Law of Demeter:** Minimal coupling between components

---

## üß† Machine Learning Details

### Reinforcement Learning Setup

**Algorithm:** Proximal Policy Optimization (PPO) with Deep Q-Networks (DQN)

**Observation Space (3 parameters):**
```csharp
sensor.AddObservation(transform.rotation.y);      // Vehicle orientation
sensor.AddObservation(_carController.velocity);   // Current speed
sensor.AddObservation(PathCrawler.currentSideDist); // Distance from path center
```

**Action Space:**
- **Discrete Actions:** Forward (1), Idle (0), Reverse (2)
- **Continuous Actions:** Steering angle [-1, 1] ‚Üí [-40¬∞, 40¬∞]

### Reward System

| Event | Reward | Purpose |
|-------|--------|---------|
| Reach path node (üü¢) | +0.2 | Encourage forward progress |
| Collision with car (üîµ) | -10.0 | Strong penalty for accidents |
| Hit sidewalk (üî¥) | -1.0 | Discourage boundary violations |
| Cross lane line | -0.1 | Keep within proper lane |
| Flip upside down | -1.0 | Penalize unstable driving |
| Existential penalty | -1/MaxStep | Motivate efficient completion |

**Episode Termination Conditions:**
- Sidewalk collision
- Vehicle-to-vehicle collision
- Upside-down orientation (>45¬∞ tilt)
- Cumulative reward < -100
- Maximum steps reached (5000)

### Hyperparameters

```yaml
trainer_type: ppo
time_horizon: 128
max_steps: 10.0e6
batch_size: 128
buffer_size: 2048
learning_rate: 3.0e-4
learning_rate_schedule: linear
epsilon: 0.2
beta: 1e-3
lambda: 0.99

network_settings:
  hidden_units: 128
  num_layers: 2
  num_epoch: 3
  vis_encode_type: simple
```

---

## üöÄ Training Process

### Parallel Training Strategy

**Acceleration Multipliers:**
- 4 training environments running simultaneously
- 5 agents per environment = **20 concurrent agents**
- Time scale: 20x speed ‚Üí **400x total training speedup**

### Progressive Curriculum

**Phase 1: Open Environment**
- Single agent in unbounded space
- Goal: Learn basic movement and reward collection
- Challenge: Initial over-optimization (agents exploiting reward system)

**Phase 2: Simple Closed Loop**
- Circular track with sidewalk boundaries
- Goal: Learn turning and boundary avoidance
- Outcome: Improved path following

**Phase 3: Complex Intersection**
- Cross-shaped intersection with multiple paths
- Goal: Handle branching decisions
- Challenge: 90¬∞ field-of-view limitation

**Phase 4: Final Configuration** ‚úÖ
- Complex intersection environment
- Enhanced 120¬∞ field-of-view (vs 90¬∞)
- Optimized grid sensor geometry
- Multiple interacting agents
- **Result:** Significant accident reduction and improved coordination

### Training Statistics

- **Total Simulations:** 28 experiments conducted
- **Successful Training Runs:** 4 major experimental phases
- **Training Duration:** 10 million steps (configurable)
- **Key Metrics Tracked:**
  - Cumulative episode rewards
  - Episode length (survival time)
  - Sidewalk collisions
  - Vehicle accidents
  - Reward ball collections
  - Lane violations

---

## üìä Results

### Performance Improvements

**Accident Reduction:**
- Significant decrease in vehicle-to-vehicle collisions over training period
- Sidewalk collision rate substantially reduced
- Episode length increased (agents survive longer)

**Learning Progress:**
- Cumulative rewards showed continuous improvement
- **No asymptote observed** ‚Üí potential for further optimization
- Agents learned complex behaviors:
  - Intersection navigation
  - Multi-vehicle coordination
  - Path following with lane discipline
  - Collision avoidance

### Key Findings

1. **Simplified Perception Works:** Grid sensor's pixelated vision sufficient for navigation
2. **Collective Behavior Emerges:** Agents coordinate without explicit communication
3. **Gradual Curriculum Essential:** Progressive difficulty prevents confusion
4. **Sensor Configuration Critical:** 120¬∞ field-of-view vs 90¬∞ made significant difference
5. **Reward Balance Matters:** Fine-tuning reward values crucial for desired behavior

---

## üõ†Ô∏è Technical Stack

### Development Environment

| Component | Version |
|-----------|---------|
| Unity Editor | 2021.3.16f1 LTS |
| ML-Agents Toolkit | 2.3.0-exp.3 |
| Python | 3.8.8 |
| ml-agents (Python) | 0.29.0 |
| ml-agents-envs | 0.29.0 |
| PyTorch | 1.13.1+cu117 |
| Communicator API | 1.5.0 |
| TensorBoard | (for training visualization) |

### System Requirements

**Hardware Used:**
- CPU: Intel Core i7-7700k @ 4.20 GHz
- RAM: 16GB
- GPU: NVIDIA GeForce GTX 1050
- OS: Windows 10

**Note:** Training is GPU-accelerated. CUDA-compatible GPU recommended for optimal performance.

### Dependencies

**Unity Packages:**
- ML-Agents Unity Package
- B√©zier Path Creator (road system)
- Grid Sensor Package (custom implementation)
- TextMesh Pro (UI)

**Python Libraries:**
```bash
pip install mlagents==0.29.0
pip install torch==1.13.1+cu117
pip install tensorboard
```

---

## üìÅ Project Structure

```
Assets/
‚îú‚îÄ‚îÄ _Scripts/
‚îÇ   ‚îú‚îÄ‚îÄ Car/                      # Vehicle agent implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TeamDriverAgent.cs    # Main RL agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MLDriverAgent.cs      # Alternative agent implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CarController.cs      # Physics-based vehicle control
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CarPercepts.cs        # Collision detection system
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ICarAgent.cs          # Agent interface
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ MLTraining/               # Training orchestration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ MLTrainingScene.cs    # Multi-agent training manager
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Pathing/                  # Navigation system
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ NodePath.cs           # Waypoint path definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PathCrawler.cs        # Path following behavior
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PathDebugDrawer.cs    # Visualization tools
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Sensors/                  # Perception system
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Grid sensor implementations
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Roads/                    # Road infrastructure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RoadPiece.cs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FourWayIntersection.cs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ThreeWayIntersection.cs
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ TrafficSignals/           # Traditional traffic control (unused in final)
‚îÇ
‚îú‚îÄ‚îÄ Resources/Prefabs/            # Reusable game objects
‚îÇ   ‚îú‚îÄ‚îÄ GridSensorTrain/          # Training environment prefabs
‚îÇ   ‚îî‚îÄ‚îÄ RoadPieces/               # Modular road components
‚îÇ
‚îú‚îÄ‚îÄ config/                       # ML-Agents training configurations
‚îÇ   ‚îî‚îÄ‚îÄ Trained models (.onnx, .pt)
‚îÇ
‚îú‚îÄ‚îÄ Demonstrations/               # Recorded agent behaviors
‚îÇ   ‚îî‚îÄ‚îÄ *.demo files
‚îÇ
‚îú‚îÄ‚îÄ Scenes/                       # Unity scenes
‚îÇ   ‚îî‚îÄ‚îÄ Training environments
‚îÇ
‚îî‚îÄ‚îÄ Models/                       # 3D assets
    ‚îî‚îÄ‚îÄ Low-poly vehicle models
```

---

## üéÆ Usage

### Setting Up Training

1. **Clone the repository:**
```bash
git clone <repository-url>
cd SmartCity
```

2. **Open in Unity:**
   - Launch Unity Hub
   - Add project (Unity 2021.3.16f1)
   - Open main training scene: `Assets/_TrainRoads/TrainScene_4.unity`

3. **Configure Python environment:**
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install ML-Agents
pip install mlagents==0.29.0
```

4. **Start training:**
```bash
mlagents-learn Assets/config/MLDriver.yaml --run-id=smart-city-run-1
```

5. **Monitor training:**
```bash
tensorboard --logdir results/
```
Navigate to `http://localhost:6006` to view real-time training metrics.

### Testing Trained Models

1. Load trained model (`.onnx` file) into agent's Behavior Parameters component
2. Set Behavior Type to "Inference Only"
3. Press Play in Unity Editor
4. Observe autonomous navigation behavior

### Manual Control (Debugging)

1. Set Behavior Type to "Heuristic Only"
2. Use keyboard controls:
   - **W/S:** Forward/Reverse
   - **A/D:** Steering
3. Helpful for validating environment setup

---

## üìñ Key Learnings & Best Practices

### From 28 Experiments

1. **Start Simple:** Begin with minimal parameters. Complex environments overwhelm untrained agents.

2. **Tune Vehicle Physics First:** Ensure car controller is responsive before ML training. Acceleration delays caused thousands of wasted training iterations.

3. **Visualize Agent Perception:** Understanding what the agent "sees" is crucial for debugging unexpected behaviors.

4. **Minimize Neural Network Inputs:** Fewer observation parameters = faster convergence. Started with many, reduced to 3 critical ones.

5. **Balance Discrete vs Continuous Actions:** Movement (forward/back) as discrete, steering as continuous worked best.

6. **Test Sensor Configuration Manually:** Validate grid sensor settings before long training runs. Small configuration errors can invalidate entire simulations.

7. **Beware of Reward Exploitation:** Agents will find creative ways to maximize rewards that don't align with intended behavior. Example: racing to first reward then jumping off platform to end episode quickly.

8. **Curriculum Learning is Essential:** Progressive difficulty from open space ‚Üí simple loop ‚Üí intersection ‚Üí multi-agent intersection.

---

## üî¨ Future Research Directions

- [ ] Increase to 100+ simultaneous agents
- [ ] More complex urban scenarios (multi-lane highways, roundabouts)
- [ ] Vehicle-to-vehicle communication protocols
- [ ] Pedestrian integration
- [ ] Dynamic obstacle avoidance (moving objects)
- [ ] Weather and visibility variations
- [ ] Transfer learning to different city layouts
- [ ] Real-world deployment considerations
- [ ] Compare with traditional traffic signal systems
- [ ] Energy efficiency optimization

---

## üìÑ Publications & References

### Thesis Documentation

- **[Master's Defense Presentation (English)](https://docs.google.com/presentation/d/1NOcKFnHB3iI_q2XVUcLx_OZ09nRyxu88X45hr9WY3zg)** - Visual overview of research and results
- **Full Thesis (Georgian):** Available upon request for academic purposes

### Key References

1. **Michael Lanham** - "Learn Unity ML-Agents Fundamentals of Unity Machine Learning" (2018), pp. 73-78
2. **Miguel Morales** - "Grokking Deep Reinforcement Learning" (2020)
3. **Chip Huyen** - "Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications" (2022), pp. 1-21
4. **Adam Streck** - "Reinforcement Learning a Self-driving Car AI in Unity" ([Article](https://towardsdatascience.com/reinforcement-learning-a-self-driving-car-ai-in-unity-60b0e7a10d9e))
5. [Unity ML-Agents GitHub](https://github.com/Unity-Technologies/ml-agents)
6. [TensorFlow Documentation](https://www.tensorflow.org/)
7. [Unity ML-Agents: Hummingbirds Course](https://learn.unity.com/course/ml-agents-hummingbirds)
8. **Ryan McLarty** - "Creating a Road System" ([Blog Post](https://ryanmclarty.me/capstone-project-update-1/))

### Assets Used
- [B√©zier Path Creator](https://assetstore.unity.com/packages/tools/utilities/b-zier-path-creator-136082)
- [Favoured Cars Pack - Low Poly (FREE)](https://assetstore.unity.com/packages/3d/vehicles/land/favoured-cars-pack-low-poly-free-226458)
- [Grid Sensor](https://github.com/mbaske/grid-sensor)

---

## ü§ù Contributing

This is an academic research project. While direct contributions may be limited, feedback and discussions are welcome:

- Report issues or bugs
- Suggest improvements to training methodology
- Share results from adapting this work
- Propose new experimental scenarios

---

## üìß Contact

**Author:** Nikoloz Astamidze  
**Institution:** Georgian Technical University  
**Program:** Master's in Computer Science  
**Year:** 2023

For academic inquiries or collaboration opportunities, please reach out through the university.

---

## üìú License

This project is released under an **Academic License**. 

- ‚úÖ Non-commercial use permitted
- ‚úÖ Academic research and study
- ‚úÖ Educational purposes
- ‚ö†Ô∏è Commercial use requires permission
- ‚ö†Ô∏è Proper citation required for derivative works

When referencing this work, please cite:
```
Astamidze, N. (2023). Autonomous Vehicle Simulation in Urban Environment. 
Master's Thesis, Georgian Technical University, Tbilisi, Georgia.
```

---

## üôè Acknowledgments

Special thanks to:
- **Professor Merab Akhobadze** - Thesis supervisor
- **Professor Mariam Chkhaidze** - Thesis reviewer
- **Diana Astamidze** - Proofreading and motivation support
- **Georgian Technical University** - Providing the academic framework
- **Unity ML-Agents Team** - For the powerful toolkit
- **Open-source community** - For the various assets and tools used

This research represents the potential of combining nature-inspired algorithms with modern machine learning to solve real-world transportation challenges. The journey from theory to working simulation validated that autonomous vehicles can indeed coordinate without centralized traffic control.

---

**‚≠ê If this project helps your research or learning, please consider starring the repository!**

---

*Built with Unity, powered by ML-Agents, inspired by nature.* üöóü§ñüåø
